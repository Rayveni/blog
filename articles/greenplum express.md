# GreenPlum базовый уровень

## Введение
**GreenPlum** - MPP реляционная *транзакционная* СУБД для хранилищ данных с гибкой горизонтальной масштабируемостью и столбцовым хранением данных на основе PostgreSQL.
![enter image description here](https://www.bigdataschool.ru/wp-content/uploads/2020/05/mpp_1.png)
Особенностью массово-параллельная архитектура (Massive parallel processing, MPP) является физическое разделение памяти узлов, объединенных в кластер. В случае MPP-СУБД каждый узел кластера работает со только своими жесткими дисками, распараллеливая операции чтения и записи данных.

**Основное предназначение**-выполнение сложных запросов аналитического профиля (OLAP) с большими объёмами структурированных данных.
Как следствие MPP архитектуры ,**GreenPlum**  обеспечивает:
 - быстроту обработки обработки данных(по сути заточен на эффективное выполния операции**full scan** -полное чтение данных из таблицы)
 -  простоту горизонтального масштабирования (с ростом числа  узлов кластера получаем линейный рост производительности), то только при условии равномерного распределения данных по узлам кластера(что не всегда тривиальная задача)
 - несмотря на то, что GreenPlum поддерживает OLTP  нагрузки(OLTP-обработка транзакций в реальном времени),при операции чтения/записи выполняется ряд дополнительных шагов(распределение данных по узлам и т.д.) что делает GreenPlum  не самым лучшим решением.

Лучше всего GreenPlum применять для:
 - построения хранилища с объёмом данных >1 ТБ
 - когда важна надежность и скорость обработки SQL запросов
 - требуется транзакционность(GreenPlum –транзакционная СУБД)

GreenPlum - opensource MPP  решение(в отличие от teradata  и v ertica).**Arenadata**-коммерческий дистрибутив greenplum ,адаптация GreenPlum  для  российского рынка. (добавление фич по администрированию и развертыванию)
| Функционал |Opensource дистрибутив  |ADB CE|ADBE EE
|--|--|--|--|
|<ul><li>Core функционал</li><li>PFX </li><li>gpbackup </li><li>Коннекторы Greenplum<->Hadoop и GreenPlum<->JDBC-источники</li></ul> | + |+ |+|
| Коннекторы GreenPlum<->Kafka и GreenPlum->ClickHouse | - |- |+|
| <ul><li>Command Center(мониторинг на уровне запросов)</li><li>оффлайн установка</li></ul> | - |- |+|
| <ul><li>Управление деплоем и апгрейдом</li><li>Расширение кластера</li><li>Мониторинг&alerting</li></ul> | - |+ |+|
| Client/Loader утилиты | - |- |RedHat B|
| Документация| English only |+ |+|
| <ul><li>Поддержка</li><li>Обучение по продуктам</li></ul> | - |- |+|
| Операционная система| Ubuntu18.04/Redhat 6/Redhat 7 |CentOS 7/Redhat 7 |CentOS 7/Redhat 7/Альт8 СП Сервер|
| Доп.консалтинговые услуги(DBAaS,Smart Start,TAM,Аудит| - |- |+|

## Архитектура и принципы
Кластер Greenplum представляет собой несколько экземпляров (инстансов, instance) объектно-реляционной базы данных PostreSQL, которые работают вместе как единая СУБД.
![enter image description here](https://arenadata.tech/wp-content/uploads/2020/12/greenplum-architecture.png)

Кластер GreenPlum состоит из 1 управляющего сервера (Master  host), где развернут главный инстанс  PostgreSQL (Master  instance),являющегося точкой входа в GreenPlum  и , присоединенных к нему (в кол-ве не менее 2 шт) сегментных сервером через interconnect(с пропускной способностью не менее 10 Гигабит)

По умолчанию используется UDP (своя реализация TCP over UDP), можно переключить на обычный TCP

Резервный мастер (**Secondary  master  instance**) — инстанс  PostgreSQL, который вручную включается в работу при отказе основного мастера.

Сервер-сегмент (**Segment  host**), который хранит и обрабатывает данные.
Segment  host  Greenplum содержат, в свою очередь, на "ячейки" :(**primary**) и зеркальные (**mirror**) сегменты ,которые представляют собой инстансы  PostgreSQL .
 Каждая ячейка сегментов Greenplum – это независимая база данных PostgreSQL, где хранится часть данных. Primary-сегмент обрабатывает локальные данные, отдавая результаты мастеру. Каждому **primary**-сегменту соответствует свое зеркало (**Mirror**  segment  instance) — инстанс  PostgreSQL, который автоматически включается в работу при отказе primary.
 На **master host** данные не хранятся.
Кластер работает со скоростью самого медленного primary сегмента, соответсвенно необходимо, чтобы:

 - сегментные сервера были одинаковы по мощности
 - кол-во primary сегментов на сегментых серверах должно быть одинаково

 
Master host обычно делают в 2ое слабее сегментного сервера.Если есть выбор между малых кол-вом мощных сегментых серверов и большим кол-вом более слабых, второй вариант как правило предпочтительнее.
![enter image description here](https://www.bigdataschool.ru/wp-content/uploads/2020/05/mpp_1.png)

 1. К кластеру можем подключиться через Postgress совместимый клиент, ODBC/JDBC  подключения к  Master Host.
 2. мастер аутентифицирует клиентские соединения и обрабатывает входящие SQL-запросы
 3. для обработки запроса в каждой базе данных сегмента создаются соответствующие процессы;
 4. после выполнения вычислений над локальными данными каждый сегмент возвращает результаты мастеру;
 5. мастер координирует результаты от сегментов и представляет конечный итог клиентской программе(при необходимости применяя агрегацию и объединение данных).
 
 ### Организация данных
 Каждая таблица присутсвует на всех сегментах кластера (в том числе и на master).Данные хранятся на primary сегментах, **на master данные не хранятся**.
 Иными словами:каждая таблица представлена в виде (N+1) таблиц на всех сегментах кластера, где N – число сегментов + 1 таблица на мастере, где нет пользовательских данных. На каждом сегменте хранится 1/N строк таблицы.
Логика распределения данных задаётся при помощи ключа распределения и необходимо стремиться к максимально равномерному распределению данных.
Соответсвенно максимальную производительность получаем когда:
 - данные распределены равномеро по primary  сегмент
 - участвующие в запросе данные находятся на одном сегменте(например при join таблиц , например с заказами и деталями заказов, "кусочки обеих таблиц" находятся на одном сегменте и могут быть соединены локально)

Понятно, что произвести вычисления локально на сегментах удаётся не всегда, и соотвественно возникают в кластере движения данных:

 - **Gather  motion**  —  объединение результатов выполнения запросов со всех сегментов в один поток (как правило, на мастер).Две таблицы, распределенные по одному ключу, который используется для джойна, выполняют все операции на сегментах, без перемещения данных. В противном случае происходит Broadcast  motion или Redistribution  motion
 - **Broadcast  motion** — каждый сегмент отправляет свою копию данных на другие сегменты. В идеальной ситуации бродкаст происходит только для маленьких таблиц.
 - **Redistribution  motion** — для соединения больших таблиц, распределенных по разным ключам, выполняется перераспределение с целью выполнения соединений локально. Для больших таблиц может быть достаточно затратной операцией.

**Broadcast**  и **Redistribution** достаточно невыгодные операции. Они выполняются при каждом запуске запроса. Рекомендуется избегать их. Увидев в плане запроса такие пункты, стоит обратить внимание на ключи распределения. Также операции distinct и union являются причиной motions.

## Типы таблиц в GreenPlum
![enter image description here](https://i.ibb.co/M6rQgqm/1.png)
Таблицы в GreenPlum  делятся на:

 - **Heap**(унаследованы от Postgres) -подходит для обработки транзакций в реальном времени (OLTP), когда данные в таблицах часто обновляются операциями INSERT, UPDATE и DELETE.  
Используется по умолчанию при создании таблиц.
 - **Append Optimized**-подходят для аналитической обработки больших массивов данных (OLAP), когда данные загружаются большими пакетами и над ними производятся только операции чтения. Операции UPDATE и DELETE не разрешены в Greenplum  над таблицами такого типа.

### Типы хранения данных
 - строчный- каждая строка хранится как непрерывная запись на диске
 ![enter image description here](https://i.onthe.io/smngoz1f6s0ltqojd.7eae6de1.jpg)
 - колоночный-все ячейки, относящиеся к колонке, как непрерывную запись.t
 Колоночное хранение данных значительно снижает затраты на чтение и запись, когда запрос обращается только к небольшому количеству столбцов из множества всех полей таблицы. Такие таблицы также лучше поддаются сжатию.![enter image description here](https://i.onthe.io/smngoz2c530uatfv8.3cca943d.jpg)

### Сжатие данных
Heap таблицы поддреживают только строчный вариант, в то время как AppendOptimized  поддерживают оба(строчный и колоночный).

 - Сжимать данные можно только в Append Optimized (AO)-таблицах.  
 - Тип истепень сжатия изменить после создания таблицы нельзя.
 - Для строковых (ROW-oriented) доступно два кодека: ZLIB и ZSTD.  
 - Для колоночных (COLUMN-oriented) доступно три: ZLIB, ZSTD, RLE. 
 - Используя сжатие, вы получаете экономию по памяти, тратя ресурс CPU. 
 - Уровень сжатия выше 5 на практике не эффективен, накладные расходы по CPU превышают пользу от экономии дискового пространства.  
 - Дляподавляющего большинства задач лучшим будет сжатие ZSTD с уровнем 1(соизмеримый ZLIB с уровнем 5).  
 - Данные на практике сжимаются хорошо,самый негативный сценарий – сжатие вдвое, обычно получается сжать в
   5-7 раз (и даже выше для банковских данных).  
- RLE иногда даёт огромный прирост, если кардинальность(уникальности значений , содержащихся в столбце) данных в столбце низкая.

## Распределение данных
Каждая таблица размещается на всех сегментах кластера. Одна строка может храниться только на одном primary-сегменте (за исключением replicated  таблиц). Необходимо стремиться к равномерному распределению данных по сегментам.
### Виды распределения

 - **DISTRIBUTED RANDOMLY** – планировщик сам раскидывает строки, используя алгоритм round-robin.  
  - **DISTRIBUTED BY** (column(s)) – строка направляется на конкретный сегмент по хешу ключа.
 Как правило,стремятся распределить по часто встречающимся в join атрибутах. 
	   - Не выбирайте для ключей дистрибуции поля, записи в которых распределены сильно неравномерно 
	   - **Не выбирайте даты**.  
	   - Не выбирайте поля, где может быть большое число значений NULL.  
	   - Не выбирайте поля, в последующем
   распределении которых вы не уверены. 
	   - Выбирайте поля, по которым с большой вероятностью будут join ить.
- **DISTRIBUTED REPLICATED** – полная копия
   таблицы хранится на каждом сегменте.Такие таблицы не могут быть партициированы, занимают больше места на кластере , но позволяют  избежать redistribution motion.

В каждой таблице (кроме replicated-таблиц) есть скрытое поле **gp_segment_id**, которое содержит content того сегмента, на котором находится запись.
Если тип распределения не указать, таблица распределится по ключу в виде первой колонки.  
**Это плохая практика, всегда указывайте политику при создании таблицы.**

#### Примеры
##### Создание таблиц распредленных случайно и по ключам
```sql
create table student60.test_table (
 id int,
 data int
)
distributed  randomly;
```
```sql
create table test_table(
    id INT, 
    num INT,
    data INT
)
distributed by (id,num);
```
##### Анализ распределения таблиц
```sql
select gp_segment_id,
       count(*) as records_per_sergment
from student60.test_table
group by gp_segment_id;
```
Все таблицы в GreenPlum распределены или реплицированы.
#### Summary
-   Есть три политики распределения: по хешу выбранного ключа, рандомное распределение и полная репликация таблицы.
- Две основные задачи при распределении:
    *  Избежать перекосов в объёме хранимых данных по сегментам.
  *  Минимизировать передачу данных между сегментами при выполнении запросов.
-    Ключ распределения выбирается также таким образом, чтобы он мог использоваться в JOIN-ах.
-   Если подходящий ключ распределения выбрать невозможно, подойдет рандомное распределение, которое эффективно решает первую задачу.
-   Реплицированные таблицы убирают перемещение данных между сегментами, но занимают больше места и медленнее заполняются и обновляются
## Партиции
![enter image description here](https://russianblogs.com/images/7/7765b3765a76ee080024a91435cd3df7.png)
**Партициирование**- разбиение  таблицы на подтаблицы.За счет этого достигается рост производительности(вместо full scan  всей таблицы идет сканирование только подтаблицы(партиции), но надо понимать, что если запросы сканируют таблицу целиком( отсутсвует условие в **where**, позволяющие выделить партиции, то  партиционирование не ускоряет, а **замедляет** производительность.
Для пользователя партициированная таблица не отличается от непартициированной, т.е. партициированная таблица отличается  от обычной лишь логикой огранизации данных.
Партиционирование может быть многоуровневым(не рекомендуется использовать в GreenPlum).
Варианты партиционирования: 

 - RANGE(диапазон)- например (если партициируем дату, можем задать дату начала , дату окончания и шаг(1 месяц) )
 - LIST(список) -явно перечисляем значения

**Примечание**:Партициированная таблица должна быть распределена по ключу или рандомно.
Поддерживает добавление внешних таблиц созданных при помощи протокола pxf(см. раздел  внешние таблицы)
**Не стоит** выбирать одно  и тоже поле для партициирования и распределения.

### default partition 
Позволяет записывать значения, не подходящие под условие партициирования в партицию по умолчанию (в противном случае insert падал бы с ошибкой).

Если default partition была создана, новые партиции не могут быть добавлены(alter table … add partition) так как в этом случае они могли частично совпасть с данными, помещенными в default партицию ранее.
### Партициирование
- Разбивать на разделы нужно только большие таблицы и лишь в том случае, если исключение или сокращение партиций может быть достигнуто на основе критериев SQL-запроса и достигается партиционированием таблицы на основе его предиката.
- Партиционирование по диапазонам(range) предпочтительнее разделения по списку(list).
- Планировщик запросов может выборочно сканировать партиционированные таблицы только если SQL-запрос содержит прямое и простое ограничение таблицы с использованием неизменяемых операторов, таких как =, <, <=,>,> = и <>.
- Выборочное сканирование(**partition elimination**) распознает стабильные (STABLE) и неизменяемые (IMMUTABLE) функции в SQL-запросе, но не определяет изменчивые (VOLATILE).
Например, условие WHERE в выражении date>CURRENT_DATE заставит планировщик запросов выборочно сканировать партиционированную таблицу. Но при использовании того же условия WHERE в выражении time>TIMEOFDAY этого НЕ произойдет. Поэтому важно убедиться, что SQL-запросы выборочно сканируют партиционированные таблицы, удаляя ненужные разделы. Это можно сделать, изучив план запроса EXPLAIN.
#### Примеры
INCLUSIVE/EXCLUSIVE позволяют включать/ исключать концы интервалов
##### RANGE INTERVAL 
```sql
CREATE TABLE sales (id int, date date, amt
decimal(10,2)) 
DISTRIBUTED BY (id)
PARTITION BY RANGE (date)
(START (date '2016-01-01') INCLUSIVE
END (date '2017-01-01') EXCLUSIVE
EVERY (INTERVAL '1 month'));
```
##### RANGE INTERVAL by INT
```sql
CREATE TABLE rank (id int, rank int, year int, 
gender char(1), count int)
DISTRIBUTED BY (id)
PARTITION BY RANGE (year)
(START (2006) END (2016) EVERY (1),
DEFAULT PARTITION extra);
```
##### RANGE INDIVIDUALLY
```sql
CREATE TABLE sales (id int, date date, amt
decimal(10,2))
DISTRIBUTED BY (id)
PARTITION BY RANGE (date)
(PARTITION Jan16 START (date '2016-01-01') INCLUSIVE,
PARTITION Feb16 START (date '2016-02-01') INCLUSIVE,
PARTITION Mar16 START (date '2016-03-01') INCLUSIVE,
PARTITION Apr16 START (date '2016-04-01') INCLUSIVE,
PARTITION May16 START (date '2016-05-01') INCLUSIVE,
PARTITION Jun16 START (date '2016-06-01') INCLUSIVE 
END (date '2016-07-01') EXCLUSIVE);
```
#####  LIST
```sql
CREATE TABLE rank (id int, rank int, year int, 
gender
char(1), count int )
DISTRIBUTED BY (id)
PARTITION BY LIST (gender)
( PARTITION girls VALUES ('F'),
PARTITION boys VALUES ('M'),
DEFAULT PARTITION other);
```
## Индексы

**Индекс** – объект в СУБД, создаваемый с целью повышения производительности при выборке данных с условиями.

Таблицы в базе данных могут иметь большое количество строк, которые хранятся в произвольном порядке, и их поиск по заданному критерию путём последовательного просмотра таблицы строка за строкой может занимать много времени. Индекс формируется из значений **одного** или **нескольких** столбцов таблицы и указателей на соответствующие строки таблицы и, таким образом, позволяет искать строки, удовлетворяющие критерию поиска. 
Ускорение работы с использованием индексов достигается в первую очередь за счёт того, что индекс имеет структуру, оптимизированную под поиск.

В текущей реализации GreenPlum  пользоваться индексами нужно крайне осторожно (операции добавления/удаления записей/ поддержки индекса стоят дорого) , как правило, механизмов **распределения** и **партициирования** достаточно для большинства задач.

 При загрузке данных в таблицу с индексами удалите индексы, загрузите данные, затем создайте 
индексы заново. Это будет быстрее, чем загружать данные в таблицу сразу.
```sql
CREATE [UNIQUE] INDEX name ON table
[USING btree|bitmap|gist|spgist|gin]
( {column_name | (expression)} [COLLATE parameter] [opclass] [ ASC | DESC ]
[ NULLS { FIRST | LAST } ] [, ...] )
[ WITH ( storage_parameter = value [, ... ] ) ]
[TABLESPACE tablespace]
[WHERE predicate] 

```
